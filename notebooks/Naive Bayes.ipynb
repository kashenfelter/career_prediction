{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import json\n",
    "from bidict import bidict\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import errno\n",
    "from ggplot import aes, geom_point, geom_line, scale_y_continuous, xlab, ylab, facet_grid, facet_wrap, facets, ggplot, theme_gray\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.externals import joblib\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from math import ceil\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use(['dark_background', 'ggplot'])\n",
    "sns.set(color_codes=True)\n",
    "sns.set_palette(sns.color_palette(\"muted\", 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsequences(sequences):\n",
    "    sub_seqs = []    \n",
    "    for seq in sequences:\n",
    "        for i in range(2,len(seq)):\n",
    "            sub_seqs.append(seq[:i])\n",
    "    return sub_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(path, file_name, model, x, targets):\n",
    "    \n",
    "    if not os.path.exists(os.path.join(path)):\n",
    "        print(\"Attempting to create directory...\")\n",
    "        try:\n",
    "            os.makedirs(os.path.join(path))\n",
    "        except OSError as exc:\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    print(f\"Saving predictions in {path}...\")\n",
    "\n",
    "    preds = model.predict_proba(x)\n",
    "    np.save(os.path.join(path, file_name + \"-preds_reduced7000.npy\"), preds)\n",
    "    np.save(os.path.join(path, file_name + \"-targets_reduced7000.npy\"), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemming(data):\n",
    "    \"\"\"data is a list of sequences\"\"\"\n",
    "    \n",
    "    stemmer = SnowballStemmer('english')\n",
    "    pattrn = re.compile(r\"[-/,\\.\\\\\\s_]\")\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    for sequence in data:\n",
    "        stemmed_seq = []        \n",
    "        for title in sequence:\n",
    "            stemmed = \"\"            \n",
    "            for token in re.split(pattrn, title):\n",
    "                if stemmed:\n",
    "                    stemmed += \" \" + stemmer.stem(token)\n",
    "                else:\n",
    "                    stemmed += stemmer.stem(token)\n",
    "            stemmed_seq.append(stemmed)\n",
    "            \n",
    "        output.append(stemmed_seq)\n",
    "\n",
    "    return [\" \".join(seq) for seq in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_sets(json_path, tokenize_titles=True, use_stemmer=True, use_sub_seq=False):\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    title_id = bidict(data[\"title_to_id\"])\n",
    "    train_data = data[\"train_data\"]\n",
    "    test_data = data[\"test_data\"]\n",
    "    \n",
    "    if use_sub_seq:\n",
    "        train_data = get_subsequences(train_data)\n",
    "        #test_data = get_subsequences(test_data)\n",
    "    \n",
    "    \n",
    "    if tokenize_titles:\n",
    "        train_seq = [[title_id.inv[i] for i in title_seq[1][:-1]] for title_seq in train_data]\n",
    "        test_seq = [[title_id.inv[i] for i in title_seq[1][:-1]] for title_seq in test_data]\n",
    "    else:\n",
    "        # replace whitespace by underscore to prevent breaking up multi word job titles\n",
    "        train_seq = [[str(i) for i in title_seq[1][:-1]] for title_seq in train_data]\n",
    "        test_seq = [[str(i) for i in title_seq[1][:-1]]  for title_seq in test_data]\n",
    "    \n",
    "    train_targets = [seq[1][-1] for seq in train_data]\n",
    "    test_targets = [seq[1][-1] for seq in test_data]\n",
    "    \n",
    "    if tokenize_titles and not use_stemmer:\n",
    "        train_text = [\" \".join(title_seq).replace(\"_\", \" \") for title_seq in train_seq]  \n",
    "        test_text = [\" \".join(title_seq).replace(\"_\", \" \") for title_seq in test_seq]\n",
    "    elif tokenize_titles and use_stemmer:\n",
    "        train_text = apply_stemming(train_seq)\n",
    "        test_text = apply_stemming(test_seq)\n",
    "    else:\n",
    "        train_text = [\" \".join(title_seq) for title_seq in train_seq]  \n",
    "        test_text = [\" \".join(title_seq) for title_seq in test_seq]\n",
    "        \n",
    "    return train_text, train_targets, test_text, test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, X_train, train_targets, X_test, test_targets):\n",
    "    # Train\n",
    "    print(f\"Training Naive Bayes...\")\n",
    "    #batches = np.array_split(X_train.toarray(), 100)\n",
    "    #target_batches = np.array_split(train_targets, 100)\n",
    "    #classes = sorted(list(set(train_targets + test_targets)))\n",
    "    #i = 0\n",
    "    model.fit(X_train, train_targets)\n",
    "    #for batch, targets in zip(batches, target_batches):\n",
    "    #    if i == 0:\n",
    "    #        model.partial_fit(batch, targets, classes=classes)\n",
    "    #    else:\n",
    "    #        model.partial_fit(batch, targets)\n",
    "\n",
    "    # Test\n",
    "    print(\"Running trained model on test dataset\")\n",
    "    predicted = model.predict(X_test)\n",
    "    acc = np.mean(predicted == test_targets)\n",
    "\n",
    "    print(\"Model Accuracy: \" + str(acc))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(data_path, tokenized_titles=False, use_stemmer=False, tf_idf=False, sub_seq=False):\n",
    "    \n",
    "    # Fetch and preprocess data\n",
    "    train, train_targets, test, test_targets = get_data_sets(data_path, tokenized_titles, use_stemmer, sub_seq)\n",
    "    \n",
    "    # Construct Models\n",
    "    multi_nb = MultinomialNB()\n",
    "    nb = BernoulliNB()\n",
    "    \n",
    "    # Construct vectorizer\n",
    "    if tokenized_titles:\n",
    "        sw = stopwords.words('english')    \n",
    "        vect = CountVectorizer(stop_words=sw)\n",
    "        vect = vect.fit(train + test)\n",
    "    else:\n",
    "        vect = CountVectorizer(token_pattern=r\"\\b\\d+\\b\")\n",
    "        vect = vect.fit(train + test)       \n",
    "    \n",
    "    print(f\"Vocab Size: {len(vect.vocabulary_)}\")\n",
    "    \n",
    "    # Construct document matices\n",
    "    X_train = vect.transform(train)\n",
    "    X_test = vect.transform(test)\n",
    "    \n",
    "    # Run Models\n",
    "    multi_nb = run_model(multi_nb, X_train, train_targets, X_test, test_targets)\n",
    "    nb = run_model(nb, X_train, train_targets, X_test, test_targets)\n",
    "\n",
    "    return multi_nb, nb, X_train, X_test, train_targets, test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(model, X, targets):\n",
    "    preds = model.predict_proba(x, targets)\n",
    "    return preds\n",
    "\n",
    "def top_k_acc(preds, targets, k=1):\n",
    "    sorted_args = (-preds).argsort(axis=1)[:,:k]\n",
    "    tt = np.tile(targets, (k,1)).T\n",
    "    acc = np.mean(np.sum(sorted_args == tt, axis=1))\n",
    "    return acc\n",
    "\n",
    "def print_top_k_accs(model, X_test, targets):\n",
    "    print(f\"acc: {top_k_acc(model, X_test, targets, k=1)[0]*100:.2f}\")\n",
    "    print(f\"top 2: {top_k_acc(model, X_test, targets, k=2)[0]*100:.2f}\")\n",
    "    print(f\"top 3: {top_k_acc(model, X_test, targets, k=3)[0]*100:.2f}\")\n",
    "    print(f\"top 4: {top_k_acc(model, X_test, targets, k=4)[0]*100:.2f}\")\n",
    "    print(f\"top 5: {top_k_acc(model, X_test, targets, k=5)[0]*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced7000_path = \"/data/rali7/Tmp/solimanz/data/datasets/reduced7000/jobid/data.json\"   \n",
    "top550_path = \"/data/rali7/Tmp/solimanz/data/datasets/top550/jobid/data.json\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"mult_nb_bow_no_stem\",\n",
    "               \"mult_nb_bow_stem\",\n",
    "               \"bern_nb_bow_no_stem\",\n",
    "               \"bern_nb_bow_stem\",\n",
    "               \"mult_nb_titles\",\n",
    "               \"bern_nb_titles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top550_models = {k: None for k in model_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = {\n",
    "    'dataset': [],\n",
    "    'top_k': [],\n",
    "    'model': [],\n",
    "    'accuracy': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dict(accs, preds, targets, dataset, model):\n",
    "    for k in range(1, 11):\n",
    "        accs['dataset'].append(dataset)\n",
    "        accs['top_k'].append(k)\n",
    "        accs['model'].append(model)\n",
    "        accs['accuracy'].append(top_k_acc(preds, targets, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 550 Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenized Job Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 326\n",
      "Training Naive Bayes...\n",
      "Running trained model on test dataset\n",
      "Model Accuracy: 0.3084449235106339\n",
      "Training Naive Bayes...\n",
      "Running trained model on test dataset\n",
      "Model Accuracy: 0.287550267401849\n"
     ]
    }
   ],
   "source": [
    "multi_nb, nb, X_train, X_test, train_targets, test_targets = run_experiment(top550_path, \n",
    "                                                                            tokenized_titles=True, \n",
    "                                                                            use_stemmer=False,\n",
    "                                                                            sub_seq=False)\n",
    "\n",
    "add_to_dict(accs, multi_nb.predict_proba(X_test), test_targets, '550_titles', \"mult_nb_bow_no_stem\")\n",
    "add_to_dict(accs, nb.predict_proba(X_test), test_targets, '550_titles', \"bern_nb_bow_no_stem\")\n",
    "\n",
    "#top550_models[\"mult_nb_bow_no_stem\"] = multi_nb.predict_proba(X_test)\n",
    "#top550_models[\"bern_nb_bow_no_stem\"] = nb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 299\n",
      "Training Naive Bayes...\n",
      "Running trained model on test dataset\n",
      "Model Accuracy: 0.30757431283943454\n",
      "Training Naive Bayes...\n",
      "Running trained model on test dataset\n",
      "Model Accuracy: 0.2869284026367066\n"
     ]
    }
   ],
   "source": [
    "multi_nb, nb, X_train, X_test, train_targets, test_targets = run_experiment(top550_path, \n",
    "                                                                            tokenized_titles=True, \n",
    "                                                                            use_stemmer=True,\n",
    "                                                                           sub_seq=False)\n",
    "\n",
    "add_to_dict(accs, multi_nb.predict_proba(X_test), test_targets, '550_titles', \"mult_nb_bow_stem\")\n",
    "add_to_dict(accs, nb.predict_proba(X_test), test_targets, '550_titles', \"bern_nb_bow_stem\")\n",
    "#top550_models[\"mult_nb_bow_stem\"] = multi_nb.predict_proba(X_test)\n",
    "#top550_models[\"bern_nb_bow_stem\"] = nb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Full Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 551\n",
      "Training Naive Bayes...\n",
      "Running trained model on test dataset\n",
      "Model Accuracy: 0.3331122258612827\n",
      "Training Naive Bayes...\n",
      "Running trained model on test dataset\n",
      "Model Accuracy: 0.27851249948177936\n"
     ]
    }
   ],
   "source": [
    "multi_nb, nb, X_train, X_test, train_targets, test_targets = run_experiment(top550_path, \n",
    "                                                                            tokenized_titles=False, \n",
    "                                                                            use_stemmer=False,\n",
    "                                                                            sub_seq=False)\n",
    "\n",
    "#add_to_dict(accs, multi_nb.predict_proba(X_test), test_targets, '550_titles', \"mult_nb_titles\")\n",
    "#add_to_dict(accs, nb.predict_proba(X_test), test_targets, '550_titles', \"bern_nb_titles\")\n",
    "\n",
    "np.save(\"/data/rali7/Tmp/solimanz/data/nb_preds/multi_nb_550.npy\", multi_nb.predict_proba(X_test))\n",
    "\n",
    "#top550_models[\"mult_nb_titles\"] = multi_nb.predict_proba(X_test)\n",
    "#top550_models[\"bern_nb_titles\"] = nb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduced 7000 Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced7k_models = {k: None for k in model_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenized Job Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 1831\n",
      "Training Naive Bayes...\n",
      "Running trained model on test dataset\n",
      "Model Accuracy: 0.1518565146570087\n",
      "Training Naive Bayes...\n",
      "Running trained model on test dataset\n",
      "Model Accuracy: 0.1305676048186445\n"
     ]
    }
   ],
   "source": [
    "multi_nb, nb, X_train, X_test, train_targets, test_targets = run_experiment(reduced7000_path, \n",
    "                                                                            tokenized_titles=True, \n",
    "                                                                            use_stemmer=False,\n",
    "                                                                            sub_seq=False)\n",
    "add_to_dict(accs, multi_nb.predict_proba(X_test), test_targets, 'reduced7k', \"mult_nb_bow_no_stem\")\n",
    "add_to_dict(accs, nb.predict_proba(X_test), test_targets, 'reduced7k', \"bern_nb_bow_no_stem\")\n",
    "\n",
    "#reduced7k_models[\"mult_nb_bow_no_stem\"] = multi_nb.predict_proba(X_test)\n",
    "#reduced7k_models[\"bern_nb_bow_no_stem\"] = nb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 1625\n",
      "Training Naive Bayes...\n",
      "Running trained model on test dataset\n",
      "Model Accuracy: 0.15068705660534967\n",
      "Training Naive Bayes...\n",
      "Running trained model on test dataset\n",
      "Model Accuracy: 0.13038860613726813\n"
     ]
    }
   ],
   "source": [
    "multi_nb, nb, X_train, X_test, train_targets, test_targets = run_experiment(reduced7000_path, \n",
    "                                                                            tokenized_titles=True, \n",
    "                                                                            use_stemmer=True,\n",
    "                                                                            sub_seq=False)\n",
    "\n",
    "add_to_dict(accs, multi_nb.predict_proba(X_test), test_targets, 'reduced7k', \"mult_nb_bow_stem\")\n",
    "add_to_dict(accs, nb.predict_proba(X_test), test_targets, 'reduced7k', \"bern_nb_bow_stem\")\n",
    "\n",
    "#reduced7k_models[\"mult_nb_bow_stem\"] = multi_nb.predict_proba(X_test)\n",
    "#reduced7k_models[\"bern_nb_bow_stem\"] = nb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Full Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 7003\n",
      "Training Naive Bayes...\n",
      "Running trained model on test dataset\n",
      "Model Accuracy: 0.16654633977529698\n",
      "Training Naive Bayes...\n",
      "Running trained model on test dataset\n",
      "Model Accuracy: 0.11268563654914408\n"
     ]
    }
   ],
   "source": [
    "multi_nb, nb, X_train, X_test, train_targets, test_targets = run_experiment(reduced7000_path, \n",
    "                                                                            tokenized_titles=False, \n",
    "                                                                            use_stemmer=False,\n",
    "                                                                            sub_seq=False)\n",
    "#add_to_dict(accs, multi_nb.predict_proba(X_test), test_targets, 'reduced7k', \"mult_nb_titles\")\n",
    "#add_to_dict(accs, nb.predict_proba(X_test), test_targets, 'reduced7k', \"bern_nb_titles\")\n",
    "np.save(\"/data/rali7/Tmp/solimanz/data/nb_preds/multi_nb_7k.npy\", multi_nb.predict_proba(X_test))\n",
    "#reduced7k_models[\"mult_nb_titles\"] = multi_nb.predict_proba(X_test)\n",
    "#reduced7k_models[\"bern_nb_titles\"] = nb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/rali7/Tmp/solimanz/data/pickles/nb_accs.pkl', 'wb') as f:\n",
    "    pickle.dump(accs, f)\n",
    "\n",
    "#with open('/data/rali7/Tmp/solimanz/data/pickles/nb_res550.pkl', 'wb') as f:\n",
    "#    pickle.dump(top550_models, f)\n",
    "#with open('/data/rali7/Tmp/solimanz/data/pickles/nb_res7k.pkl', 'wb') as f:\n",
    "#    pickle.dump(reduced7k_models, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/rali7/Tmp/solimanz/data/pickles/nb_accs.pkl', 'rb') as f:\n",
    "    accs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path550 = \"/data/rali7/Tmp/solimanz/data/datasets/top550/\"\n",
    "path7k = \"/data/rali7/Tmp/solimanz/data/datasets/reduced7000/\"\n",
    "\n",
    "# Load data dicts\n",
    "with open(os.path.join(path550, \"jobid\", \"data.json\"), \"r\") as f:\n",
    "    data550 = json.load(f)\n",
    "with open(os.path.join(path7k, \"jobid\", \"data.json\"), \"r\") as f:\n",
    "    data7k = json.load(f)\n",
    "\n",
    "targets550 = [d[1][-1] for d in data550['test_data']]\n",
    "targets7k = [d[1][-1] for d in data7k['test_data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(res, targets):\n",
    "    ks = list(range(1,11))\n",
    "    #df_dict = {\"model\": [], \"acc\": [], \"top_2_acc\": [], \"top_3_acc\": [], \"top_4_acc\": [], \"top_5_acc\": []}\n",
    "    df_dict = {\"model\": [], \"top_k\": [], \"value\": []}\n",
    "\n",
    "    for name, preds in res.items():\n",
    "        for k in ks:\n",
    "            df_dict[\"model\"].append(name)\n",
    "            df_dict[\"top_k\"].append(k)\n",
    "            df_dict[\"value\"].append(round(top_k_acc(preds, targets, k), 5)*100)\n",
    "    return pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df550 = get_dataframe(top550_models, targets550)\n",
    "df7k = get_dataframe(reduced7k_models, targets7k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"top_k\"] = df[\"top_k\"].apply(lambda x: f\"Top {x} accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_df = pd.DataFrame(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ggplot(aes(x='top_k', y='accuracy', color='model'), data=accs_df) \\\n",
    "+ geom_point(size=100, alpha=0.5) \\\n",
    "+ geom_line() \\\n",
    "+ facet_wrap('dataset', nrow=1, ncol=2) \\\n",
    "+ xlab(\"Top K predictions\") \\\n",
    "+ ylab(\"Accuracy\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs550 = accs_df[accs_df.dataset == '550_titles']\n",
    "accs7k = accs_df[accs_df.dataset == 'reduced7k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-03a9f5b94fc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.dpi'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lines.linewidth'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'top_k'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccs550\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'top_k'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccs7k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.rcParams['figure.dpi'] = 160\n",
    "plt.rcParams['lines.linewidth'] = 0.8\n",
    "fig, ax = plt.subplots(figsize=(13,4), ncols=2, nrows=1)\n",
    "sns.pointplot(x='top_k', y='accuracy', hue='model', data=accs550, ax=ax[0])\n",
    "sns.pointplot(x='top_k', y='accuracy', hue='model', data=accs7k, ax=ax[1])\n",
    "ax[0].set_title('550 Titles Dataset')\n",
    "ax[1].set_title('7k Titles Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('../../../Dropbox/thesis/thesis/images/nb_res.png', dpi=180, facecolor=fig.get_facecolor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
