{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/solimanz/development/career_prediction_gpu/lib/python3.6/site-packages/ggplot/utils.py:81: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access Timestamp as pandas.Timestamp\n",
      "  pd.tslib.Timestamp,\n",
      "/u/solimanz/development/career_prediction_gpu/lib/python3.6/site-packages/ggplot/stats/smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n",
      "/u/solimanz/development/career_prediction_gpu/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from ggplot import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from bidict import bidict\n",
    "from collections import Counter, defaultdict\n",
    "from bidict import bidict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use(['dark_background', 'ggplot'])\n",
    "sns.set(color_codes=True)\n",
    "sns.set_palette(sns.color_palette('dark'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path550 = \"/data/rali7/Tmp/solimanz/data/datasets/top550/\"\n",
    "path7k = \"/data/rali7/Tmp/solimanz/data/datasets/reduced7000/\"\n",
    "\n",
    "# Load data dicts\n",
    "with open(os.path.join(path550, \"jobid\", \"data.json\"), \"r\") as f:\n",
    "    data550 = json.load(f)\n",
    "with open(os.path.join(path7k, \"jobid\", \"data.json\"), \"r\") as f:\n",
    "    data7k = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id550 = bidict(data550['title_to_id'])\n",
    "title_id7k = bidict(data7k['title_to_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels550_counts = Counter([title_id550.inv[d[1][-1]] for d in data550['train_data']])\n",
    "labels7k_counts = Counter([title_id7k.inv[d[1][-1]] for d in data7k['train_data']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\s+|\\W', gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels550 = list(labels550_counts.keys())\n",
    "labels7k = list(labels7k_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english') + ['', '&'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab550 = set(tokenizer.tokenize(\" \".join(labels550)))\n",
    "vocab7k = set(tokenizer.tokenize(\" \".join(labels7k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab550 = vocab550 - sw\n",
    "vocab7k = vocab7k - sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible labels in 550 dataset: 326\n",
      "Number of possible labels in 7k dataset: 1856\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of possible labels in 550 dataset: {len(vocab550)}\")\n",
    "print(f\"Number of possible labels in 7k dataset: {len(vocab7k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab550 = [w for w in tokenizer.tokenize(\" \".join([title_id550.inv[d[1][-1]] for d in data550['train_data']])) if w not in sw]\n",
    "vocab7k = [w for w in tokenizer.tokenize(\" \".join([title_id7k.inv[d[1][-1]] for d in data7k['train_data']])) if w not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_counter(counts):\n",
    "    total = sum(counts.values(), 0)\n",
    "    for key in counts:\n",
    "        counts[key] /= total\n",
    "        counts[key] *= 100\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = normalize_counter(Counter(vocab550))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(label_counts):\n",
    "    df_dict = defaultdict(list)\n",
    "    sw = set(stopwords.words('english'))\n",
    "    for title, count in label_counts.items():\n",
    "        df_dict['title'].append(title)\n",
    "        df_dict['count'].append(count)        \n",
    "        df_dict['n_words'].append(len([w for w in word_tokenize(title) if w not in sw]))\n",
    "    return pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df550 = make_df(labels550_counts)\n",
    "df7k = make_df(labels7k_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Word Tokens to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id_550 = bidict({label: id_ for id_, label in enumerate(vocab550)})\n",
    "label_id_7k = bidict({label: id_ for id_, label in enumerate(vocab7k)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_titles_to_labels(title_id, label_id):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\s+|\\W', gaps=True)\n",
    "    title_labels = dict()\n",
    "    \n",
    "    for title in title_id.keys():\n",
    "        toks = tokenizer.tokenize(title)\n",
    "        labels = [label_id[t] for t in toks if t in label_id]\n",
    "        title_labels[title_id[title]] = labels\n",
    "    return title_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_labels_550 = map_titles_to_labels(title_id550, label_id_550)\n",
    "title_labels_7k = map_titles_to_labels(title_id7k, label_id_7k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data_dict, label_id, title_labels):\n",
    "    data = dict()\n",
    "    data['title_to_id'] = dict(data_dict['title_to_id'])\n",
    "    data['label_id'] = dict(label_id)\n",
    "    \n",
    "    data['train_data'] = [[d[0], d[1][:-1]] for d in data_dict['train_data']]\n",
    "    data['train_targets'] = [[d[0], title_labels[d[1][-1]]] for d in data_dict['train_data']]\n",
    "    \n",
    "    data['test_data'] = [[d[0], d[1][:-1]] for d in data_dict['test_data']]\n",
    "    data['test_targets'] = [[d[0], title_labels[d[1][-1]]] for d in data_dict['test_data']]\n",
    "    \n",
    "    data['maximum_seq_len'] = data_dict['maximum_seq_len']\n",
    "    data['n_labels'] = len(label_id)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_550 = make_dataset(data550, label_id_550, title_labels_550)\n",
    "data_7k = make_dataset(data7k, label_id_7k, title_labels_7k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, dataset='top550'):\n",
    "    with open(f\"/data/rali7/Tmp/solimanz/data/datasets/multilabel/{dataset}/data.json\", \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(data_550, 'top550')\n",
    "save_data(data_7k, 'reduced7k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros((3,5), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[1,[2,0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(path):\n",
    "    batch = 0\n",
    "    name = os.path.basename(path)\n",
    "    while os.path.exists(os.path.join(path, f\"{name}_batch_{batch}.npy\")):\n",
    "        with open(os.path.join(path, f\"{name}_batch_{batch}.npy\"), \"rb\") as f:\n",
    "            matrix = np.load(f)\n",
    "        batch += 1\n",
    "        yield matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/rali7/Tmp/solimanz/data/multilabel_model_predictions/\"\n",
    "multi_label = dict()\n",
    "datasets = ['top550']\n",
    "for ds in datasets:\n",
    "    mats = []\n",
    "    gen = next_batch(os.path.join(path, ds, 'targets'))\n",
    "    for preds in gen:\n",
    "        mats.append(preds)\n",
    "    multi_label[ds] = np.concatenate(mats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = multi_label['top550']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = multi_label['top550']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.zeros(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(data_550['test_targets']):\n",
    "    ts[i][d[1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0,     0,     1, ..., 24120, 24120, 24120]),\n",
       " array([107, 248,  21, ...,  22, 266, 325]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(targets != ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
