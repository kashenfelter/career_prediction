{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyfasttext import FastText\n",
    "from pprint import pprint\n",
    "import os\n",
    "import json\n",
    "from bidict import bidict\n",
    "import random\n",
    "from math import ceil, floor\n",
    "import pickle\n",
    "from bson.objectid import ObjectId\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_pickle(\"/data/rali7/Tmp/solimanz/data/pickles/excerpt-2017-02-20_transformed.pkl\")\n",
    "df = pd.read_pickle(\"/data/rali7/Tmp/solimanz/data/pickles/excerpt-2017-02-20_reduced.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_counts = df.transformed.value_counts()\n",
    "top = func_counts[:550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second cleaning method\n",
    "func_counts = df.reduced.value_counts()\n",
    "top = func_counts[:7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ids = df[~df.reduced.isin(top.index)][\"_id\"].unique()\n",
    "all_ids = df[\"_id\"].unique()\n",
    "dataset_ids = list(set(all_ids) - set(bad_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of entire dataset: 837910\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of entire dataset: {len(dataset_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = ceil(0.8 * len(dataset_ids))\n",
    "random.shuffle(dataset_ids)\n",
    "train_ids = dataset_ids[:train_size]\n",
    "test_ids = dataset_ids[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mapping between job title id and string representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = top.index.values\n",
    "title_id = {title: i for i, title in enumerate(job_titles)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep a record of the training and testing IDs for later experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/top7000/train_ids.pkl\", \"wb\")as f:\n",
    "    pickle.dump(file=f, obj=train_ids)\n",
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/top7000/test_ids.pkl\", \"wb\")as f:\n",
    "    pickle.dump(file=f, obj=test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS1: Simple Job Titles Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_path = \"/data/rali7/Tmp/solimanz/data/datasets/top7000/1/\"\n",
    "ds1_file_name = \"title_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df._id.isin(dataset_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_series = df.groupby('_id')['reduced'].apply(lambda x: list(reversed(list(x))))\n",
    "#func_series = df.groupby('_id')['transformed'].apply(lambda x: list(reversed(list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>end_date</th>\n",
       "      <th>function</th>\n",
       "      <th>industry</th>\n",
       "      <th>job_index</th>\n",
       "      <th>place</th>\n",
       "      <th>start_date</th>\n",
       "      <th>transformed</th>\n",
       "      <th>reduced</th>\n",
       "      <th>title_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>52b31a880b045119318b45be</td>\n",
       "      <td>Ashtanga Yoga Victoria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Owner</td>\n",
       "      <td>Health, Wellness and Fitness</td>\n",
       "      <td>0</td>\n",
       "      <td>Victoria British Columbia Canada</td>\n",
       "      <td>2009-08</td>\n",
       "      <td>owner</td>\n",
       "      <td>owner</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>52b31a880b045119318b45be</td>\n",
       "      <td>Living Breathing Yoga</td>\n",
       "      <td>2009</td>\n",
       "      <td>Teacher / Director</td>\n",
       "      <td>Health, Wellness and Fitness</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005</td>\n",
       "      <td>teacher/director</td>\n",
       "      <td>director</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>52b31a880b045119318b45be</td>\n",
       "      <td>Yoga Thailand</td>\n",
       "      <td>2007</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Health, Wellness and Fitness</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004</td>\n",
       "      <td>teacher</td>\n",
       "      <td>teacher</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>52b31a880b045119318b45be</td>\n",
       "      <td>Yoga Shala Calgary</td>\n",
       "      <td>2011</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Health, Wellness and Fitness</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003</td>\n",
       "      <td>teacher</td>\n",
       "      <td>teacher</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>52b31a880b045119318b45be</td>\n",
       "      <td>Yoga Passage Calgary</td>\n",
       "      <td>2004</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Health, Wellness and Fitness</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002</td>\n",
       "      <td>teacher</td>\n",
       "      <td>teacher</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          _id            company_name end_date  \\\n",
       "462  52b31a880b045119318b45be  Ashtanga Yoga Victoria      NaN   \n",
       "463  52b31a880b045119318b45be   Living Breathing Yoga     2009   \n",
       "464  52b31a880b045119318b45be           Yoga Thailand     2007   \n",
       "465  52b31a880b045119318b45be      Yoga Shala Calgary     2011   \n",
       "466  52b31a880b045119318b45be    Yoga Passage Calgary     2004   \n",
       "\n",
       "               function                      industry  job_index  \\\n",
       "462               Owner  Health, Wellness and Fitness          0   \n",
       "463  Teacher / Director  Health, Wellness and Fitness          1   \n",
       "464             Teacher  Health, Wellness and Fitness          2   \n",
       "465             Teacher  Health, Wellness and Fitness          3   \n",
       "466             Teacher  Health, Wellness and Fitness          4   \n",
       "\n",
       "                                place start_date       transformed   reduced  \\\n",
       "462  Victoria British Columbia Canada    2009-08             owner     owner   \n",
       "463                               NaN       2005  teacher/director  director   \n",
       "464                               NaN       2004           teacher   teacher   \n",
       "465                               NaN       2003           teacher   teacher   \n",
       "466                               NaN       2002           teacher   teacher   \n",
       "\n",
       "     title_rank  \n",
       "462           1  \n",
       "463           2  \n",
       "464          30  \n",
       "465          30  \n",
       "466          30  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df._id == '52b31a880b045119318b45be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_id\n",
       "52b31a870b045119318b456c    [chief operating officer, chief operating offi...\n",
       "52b31a870b045119318b4575    [owner, chief executive officer, chief executi...\n",
       "52b31a880b045119318b4597    [developer, marketing associate, marketing, ma...\n",
       "52b31a880b045119318b45aa    [president, president, president, president, p...\n",
       "52b31a880b045119318b45be         [teacher, teacher, teacher, director, owner]\n",
       "52b31a890b045119318b45db    [human resources manager, human resources mana...\n",
       "52b31a890b045119318b45dc    [director, manager, sales, sales, director of ...\n",
       "52b31a890b045119318b45f6    [partner, general manager, partner, general ma...\n",
       "52b31a890b045119318b460f    [business analyst, managing director, operatio...\n",
       "52b31a890b045119318b4610    [owner, owner, owner, owner, creative director...\n",
       "52b31a890b045119318b461b    [account coordinator, senior consultant, senio...\n",
       "52b31a8a0b045119318b4631    [senior systems analyst, chief executive offic...\n",
       "52b31a8a0b045119318b4637    [trainer, senior consultant, management consul...\n",
       "52b31a8a0b045119318b4642                  [partner, adjunct professor, chair]\n",
       "52b31a8a0b045119318b4650    [sales, account executive, business developmen...\n",
       "52b31a8b0b045119318b467c    [account representative, marketing manager, sa...\n",
       "52b31a8b0b045119318b468c    [sales manager, sales manager, partner, presid...\n",
       "52b31a8c0b045119318b46c2                    [principal, director, instructor]\n",
       "52b31a8c0b045119318b46c6    [project manager, program manager, program man...\n",
       "52b31a8f0b045119318b4778    [senior accountant, vice president finance, re...\n",
       "52b31a8f0b045119318b4789    [auditor, internal auditor, financial analyst,...\n",
       "52b31a8f0b045119318b4796    [account director, president, member, executiv...\n",
       "52b31a940b045119318b47ba    [various, branch manager, professor, professor...\n",
       "52b31a940b045119318b47be                 [director, vice president, director]\n",
       "52b31a940b045119318b47c3    [human resources, human resources advisor, man...\n",
       "52b31a940b045119318b47d7    [intern, information technology manager, senio...\n",
       "52b31a940b045119318b47d8    [producer, producer, vice president, senior vi...\n",
       "52b31a940b045119318b47dc                     [professor, professor, director]\n",
       "52b31a940b045119318b47e0    [associate, associate, manager, partner, partn...\n",
       "52b31a940b045119318b47f0    [operations manager, general manager, general ...\n",
       "                                                  ...                        \n",
       "579efab39d15a96d778c2466    [inside sales representative, communications o...\n",
       "579efab59d15a96d778c2471    [heavy equipment operator, heavy equipment ope...\n",
       "579efadd9d15a96d778c24d9              [manager, self employed, self employed]\n",
       "579efb159d15a96d778c2592    [account manager, account manager, management ...\n",
       "579efb159d15a96d778c2594    [manager, manager, board member, human resourc...\n",
       "579efb169d15a96d778c259c    [graphic designer, research assistant, chief e...\n",
       "579efb229d15a96d778c25c6    [machinist, machinist, machinist, machinist, m...\n",
       "579efb389d15a96d778c2634            [technical support, president, president]\n",
       "579efb669d15a96d778c26bd            [articling student, associate, associate]\n",
       "579efb759d15a96d778c26ee    [assistant manager, volunteer, volunteer, admi...\n",
       "579efb879d15a96d778c2734    [bilingual customer service representative, re...\n",
       "579efb889d15a96d778c273a    [software engineer, software developer, progra...\n",
       "579efb889d15a96d778c273b    [programmer, research assistant, software engi...\n",
       "579efb8c9d15a96d778c2743    [customer service representative, server, sale...\n",
       "579efb8f9d15a96d778c274f    [editorial intern, editorial intern, photograp...\n",
       "579efba29d15a96d778c2797    [private tutor, research assistant, research a...\n",
       "579efba69d15a96d778c27a7    [shift supervisor, personal support worker, su...\n",
       "579efbb09d15a96d778c27ca    [research associate, vice president, president...\n",
       "579efbc29d15a96d778c2807                     [summer student, barista, tutor]\n",
       "579efbc49d15a96d778c2817    [accounting assistant, bookkeeper, accounting ...\n",
       "579efc979d15a96d778c2858                     [cashier, teacher, receptionist]\n",
       "579efcd79d15a96d778c2860    [accounting manager, senior financial analyst,...\n",
       "579efe869d15a96d778c28b5                   [instructor, lecturer, researcher]\n",
       "579f01e09d15a96d778c297b                   [office manager, manager, manager]\n",
       "579f02d49d15a96d778c29a8    [mechanical engineer, graduate research assist...\n",
       "579f03129d15a96d778c29b5                           [barista, intern, barista]\n",
       "579f07309d15a96d778c2a73         [sales, account manager, marketing director]\n",
       "579f077c9d15a96d778c2a7e    [lecturer, software engineer, senior software ...\n",
       "579f098b9d15a96d778c2ac9    [operations manager, account manager, financia...\n",
       "579f0ce79d15a96d778c2b33    [server, supervisor, receptionist, logistics c...\n",
       "Name: reduced, Length: 257573, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [[title_id[title] for title in func_series[i]] for i in train_ids]\n",
    "test_data = [[title_id[title] for title in func_series[i]] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2443751089a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmax_train_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmax_test_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_data])\n",
    "max_test_seq = max([len(seq) for seq in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Longest Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 53\n",
      "Maximum length of test sequences: 65\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq),\n",
    "        'n_lables': len(top)\n",
    "    }\n",
    "\n",
    "with open(os.path.join(ds1_path, f\"{ds1_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS2: Add Start of Sequence tags to DS1 Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2_path = \"/data/rali7/Tmp/solimanz/data/datasets/top7000/2/\"\n",
    "ds2_file_name = \"title_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ds1_path, f\"{ds1_file_name}.json\"), 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = data[\"title_to_id\"]\n",
    "train_data = data[\"train_data\"] \n",
    "test_data = data[\"test_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id['<START>'] = len(title_id)\n",
    "start_tag = title_id['<START>']\n",
    "_ =[seq.insert(0, start_tag) for seq in train_data]\n",
    "_ =[seq.insert(0, start_tag) for seq in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_data])\n",
    "max_test_seq = max([len(seq) for seq in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Longest Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 54\n",
      "Maximum length of test sequences: 66\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq)\n",
    "    }\n",
    "with open(os.path.join(ds2_path, f\"{ds2_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS3: Job Sequences as Sequences of Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we represent our job experience sequences as sequences of bag-of-words vectors that we will feed to the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3_path = \"/data/rali7/Tmp/solimanz/data/datasets/3/\"\n",
    "ds3_file_name = \"title_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/train_ids.pkl\", \"rb\")as f:\n",
    "    train_ids = pickle.load(file=f)\n",
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/test_ids.pkl\", \"rb\")as f:\n",
    "    test_ids = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = \" \".join(top_550.index.values)\n",
    "tokens = re.split(r\"[-/,\\.\\\\\\s]\", joined)\n",
    "token_counts = Counter(tokens)\n",
    "vocab = set(token_counts.keys())\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "for word in sw:\n",
    "    if word in vocab:\n",
    "        vocab.remove(word)\n",
    "if '' in vocab:\n",
    "    vocab.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_id = {l: i for i, l in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 329\n"
     ]
    }
   ],
   "source": [
    "voc_size = len(vocab_id)\n",
    "bow = {}\n",
    "print(f\"Vocabulary Size: {voc_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in func_counts.index:\n",
    "    tokens = re.split(r\"[-/,\\.\\\\\\s]\", title)\n",
    "    token_indices = [vocab_id[tok] for tok in tokens if tok in vocab_id]\n",
    "    bow[title] = sorted(token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df._id.isin(dataset_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_series = df.groupby('_id')['transformed'].apply(lambda x: list(reversed(list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = [[title for title in func_series[i]] for i in train_ids]\n",
    "test_seqs = [[title for title in func_series[i]] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "train_inputs = [[bow[title] for title in seq[:-1]] for seq in train_seqs] \n",
    "test_inputs = [[bow[title] for title in seq[:-1]] for seq in test_seqs]\n",
    "\n",
    "# Targets\n",
    "train_targets = [[title_id[title] for title in seq[1:]] for seq in train_seqs]\n",
    "test_targets = [[title_id[title] for title in seq[1:]] for seq in test_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_seqs])\n",
    "max_test_seq = max([len(seq) for seq in test_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 32\n",
      "Maximum length of test sequences: 19\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'title_to_bow': bow,\n",
    "        'vocab_id': vocab_id,\n",
    "        'train_inputs': train_inputs,\n",
    "        'test_inputs': test_inputs,\n",
    "        'train_targets': train_targets,\n",
    "        'test_targets': test_targets,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq)\n",
    "    }\n",
    "with open(os.path.join(ds3_path, f\"{ds3_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS4: Add Job Duration to DS3 Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds4_path = \"/data/rali7/Tmp/solimanz/data/datasets/4/\"\n",
    "ds4_file_name = \"title_sequences_durations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df._id.isin(dataset_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_series = df.groupby('_id')['transformed'].apply(lambda x: list(reversed(list(x))))\n",
    "duration_series = df.groupby('_id')['duration'].apply(lambda x: list(reversed(list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['director', 'instructor', 'instructor', 'substitute teacher']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_series['52b31c980b045119318b9d64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.17, 0.0, 5.83, 11.75]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_series[324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = [[title for title in func_series[i]] for i in train_ids]\n",
    "test_seqs = [[title for title in func_series[i]] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs we will concatenate the duration value with the corresponding bow in the batcher\n",
    "train_inputs = [[bow[title] for title in seq[:-1]] for seq in train_seqs]\n",
    "train_duration = [[dur for dur in duration_series[i][:-1]] for i in train_ids]\n",
    "test_inputs = [[bow[title] for title in seq[:-1]] for seq in test_seqs]\n",
    "test_duration = [[dur for dur in duration_series[i][:-1]] for i in test_ids]\n",
    "\n",
    "# Targets\n",
    "train_targets = [[title_id[title] for title in seq[1:]] for seq in train_seqs]\n",
    "test_targets = [[title_id[title] for title in seq[1:]] for seq in test_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_seqs])\n",
    "max_test_seq = max([len(seq) for seq in test_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 32\n",
      "Maximum length of test sequences: 22\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'title_to_bow': bow,\n",
    "        'vocab_id': vocab_id,\n",
    "        'train_inputs': train_inputs,\n",
    "        'train_durations': train_duration,\n",
    "        'test_inputs': test_inputs,\n",
    "        'test_duration': test_duration,\n",
    "        'train_targets': train_targets,\n",
    "        'test_targets': test_targets,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq)\n",
    "    \n",
    "    }\n",
    "with open(os.path.join(ds4_path, f\"{ds4_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS5: Input Vectors as Word Embeddings Mutli Label Targets(Fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds5_path = \"/data/rali7/Tmp/solimanz/data/datasets/5/\"\n",
    "ds5_file_name = \"title_embedding_sequences_multi_label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ds1_path, f\"{ds1_file_name}.json\"), 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(\"/data/rali7/Tmp/solimanz/data/wikipedia/wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = bidict(data[\"title_to_id\"])\n",
    "train_seqs = data[\"train_data\"]\n",
    "test_seqs = data[\"test_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(title_id), emb_dim), dtype=np.float32)\n",
    "\n",
    "for title in title_id.keys():\n",
    "    if len(title.split(\" \")) == 1:\n",
    "        vec = model.get_numpy_vector(title)\n",
    "    else:\n",
    "        vec = model.get_sentence_vector(title)\n",
    "    embeddings[title_id[title], :] = vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Targets will be represented as multilabel vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "train_inputs = [[title for title in seq[:-1]] for seq in train_seqs] \n",
    "test_inputs = [[title for title in seq[:-1]] for seq in test_seqs]\n",
    "\n",
    "# Targets\n",
    "train_targets = [[title_id.inv[title] for title in seq[1:]] for seq in train_seqs]\n",
    "test_targets = [[title_id.inv[title] for title in seq[1:]] for seq in test_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = \" \".join(title_id.keys())\n",
    "tokens = Counter(re.split(r\"[-/,\\.\\\\\\s_]\", titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = set(tokens.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "sw.append('')\n",
    "sw.append(' ')\n",
    "for word in sw:\n",
    "    if word in tokens:\n",
    "        tokens.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = {tok: i for i, tok in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 326\n",
      "Number of Titles: 550\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Tokens: {len(token_id)}\")\n",
    "print(f\"Number of Titles: {len(title_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = [[[token_id[tok] for tok in re.split(r\"[-/,\\.\\\\\\s_]\", title) if tok in token_id] \n",
    "  for title in seq] for seq in train_targets]\n",
    "test_targets = [[[token_id[tok] for tok in re.split(r\"[-/,\\.\\\\\\s_]\", title) if tok in token_id] \n",
    "  for title in seq] for seq in test_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_seqs])\n",
    "max_test_seq = max([len(seq) for seq in test_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 32\n",
      "Maximum length of test sequences: 22\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': dict(title_id),\n",
    "        'token_id': token_id,\n",
    "        'train_inputs': train_inputs,\n",
    "        'test_inputs': test_inputs,\n",
    "        'train_targets': train_targets,\n",
    "        'test_targets': test_targets,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq),\n",
    "        'emb_dim': emb_dim\n",
    "    \n",
    "    }\n",
    "with open(os.path.join(ds5_path, f\"{ds5_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(ds5_path, \"embeddings_small.npy\"), embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS6: Multi Label Data Representation (Larger Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds6_path = \"/data/rali7/Tmp/solimanz/data/datasets/6/\"\n",
    "ds6_file_name = \"big_title_embedding_sequences_multi_label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/train_ids.pkl\", \"rb\")as f:\n",
    "    train_ids = pickle.load(file=f)\n",
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/test_ids.pkl\", \"rb\")as f:\n",
    "    test_ids = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(s):\n",
    "    if pd.isna(s) or pd.isnull(s):\n",
    "        return np.nan\n",
    "    toks = re.split(r\"[-/,\\.\\\\\\s_]\", s)\n",
    "    token_indices = [tok for tok in toks if tok in token_id]\n",
    "    if token_indices:\n",
    "        return \" \".join(token_indices)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = set(re.split(r\"[-/,\\.\\\\\\s_]\", \" \".join(top_550.index.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "sw.append('')\n",
    "sw.append(' ')\n",
    "for word in sw:\n",
    "    if word in tokens:\n",
    "        tokens.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = {tok: i for i, tok in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reduced'] = df['transformed'].apply(reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2198993"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = df._id.unique()\n",
    "bad_ids = df[pd.isna(df.reduced)]._id.unique()\n",
    "len(all_ids) - len(bad_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df._id.isin(bad_ids)]\n",
    "titles = df.reduced.unique()\n",
    "dataset_ids = df._id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = {title: i for i, title in enumerate(titles)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_size = ceil(0.8 * len(dataset_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset_ids)\n",
    "train_ids = dataset_ids[:train_set_size]\n",
    "test_ids = dataset_ids[train_set_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/train_ids_big.pkl\", \"wb\")as f:\n",
    "    pickle.dump(file=f, obj=train_ids)\n",
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/test_ids_big.pkl\", \"wb\")as f:\n",
    "    pickle.dump(file=f, obj=test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_series = df.groupby('_id')['reduced'].apply(lambda x: list(reversed(list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [[title_id[title] for title in func_series[i]] for i in train_ids]\n",
    "test_data = [[title_id[title] for title in func_series[i]] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_data])\n",
    "max_test_seq = max([len(seq) for seq in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Longest Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 89\n",
      "Maximum length of test sequences: 56\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 326\n",
      "Number of Titles: 731408\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Tokens: {len(token_id)}\")\n",
    "print(f\"Number of Titles: {len(title_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(\"/data/rali7/Tmp/solimanz/data/wikipedia/wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = model.get_numpy_vector(\"engineer\")\n",
    "senior = model.get_numpy_vector(\"senior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(eng - senior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(title_id), emb_dim), dtype=np.float32)\n",
    "\n",
    "for title in title_id.keys():\n",
    "    vec = model.get_sentence_vector(title)\n",
    "    embeddings[title_id[title], :] = vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Targets will be represented as multilabel vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = bidict(title_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "train_inputs = [[title for title in seq[:-1]] for seq in train_data] \n",
    "test_inputs = [[title for title in seq[:-1]] for seq in test_data]\n",
    "\n",
    "# Targets\n",
    "train_targets = [[title_id.inv[title] for title in seq[1:]] for seq in train_data]\n",
    "test_targets = [[title_id.inv[title] for title in seq[1:]] for seq in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = [[[token_id[tok] for tok in re.split(r\"[-/,\\.\\\\\\s_]\", title) if tok in token_id] \n",
    "  for title in seq] for seq in train_targets]\n",
    "test_targets = [[[token_id[tok] for tok in re.split(r\"[-/,\\.\\\\\\s_]\", title) if tok in token_id] \n",
    "  for title in seq] for seq in test_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': dict(title_id),\n",
    "        'token_id': token_id,\n",
    "        'train_inputs': train_inputs,\n",
    "        'test_inputs': test_inputs,\n",
    "        'train_targets': train_targets,\n",
    "        'test_targets': test_targets,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq),\n",
    "        'emb_dim': emb_dim\n",
    "    \n",
    "    }\n",
    "with open(os.path.join(ds6_path, f\"{ds6_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(ds6_path, \"embeddings_big.npy\"), embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['co owner', 'co owner', 'owner'],\n",
       " ['associate', 'partner', 'partner'],\n",
       " ['analyst',\n",
       "  'information technology consultant',\n",
       "  'project manager',\n",
       "  'program manager',\n",
       "  'director',\n",
       "  'director',\n",
       "  'director'],\n",
       " ['educator',\n",
       "  'project coordinator',\n",
       "  'associate project manager',\n",
       "  'project manager'],\n",
       " ['investment representative',\n",
       "  'operations associate',\n",
       "  'financial service representative'],\n",
       " ['sales associate', 'sales associate', 'financial services associate'],\n",
       " ['field operator', 'field sales', 'field supervisor'],\n",
       " ['security guard',\n",
       "  'sales associate',\n",
       "  'receptionist',\n",
       "  'security',\n",
       "  'security officer',\n",
       "  'administrative support assistant'],\n",
       " ['facilitator', 'counsellor', 'youth coordinator', 'addictions counsellor'],\n",
       " ['music educator', 'music educator', 'educator']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list(func_series.values), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adsf.sdf']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"adsf.sdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((550, 329))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "def dump(path, serializer, obj):\n",
    "    \"\"\"\n",
    "    Saves 'obj' to 'path' using 'serializer' (either pickle or json)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(os.path.dirname(path)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(path))\n",
    "        except OSError as exc:  # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    mode = \"wb\" if serializer.__name__ == \"pickle\" else \"w\"\n",
    "    with open(path, mode) as f:\n",
    "        serializer.dump(f, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4e78497fa96c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ass.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dfdfd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-d4e4d39cb2cc>\u001b[0m in \u001b[0;36mdump\u001b[0;34m(path, serializer, obj)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Guard against race condition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEEXIST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.3/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "dump('ass.txt', pickle, 'dfdfd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/rali7/Tmp/solimanz/data/datasets/reduced7000'\n",
    "\n",
    "with open(os.path.join(path, 'bow', 'data.json'), 'r') as f:\n",
    "    bow_dat = json.load(f)\n",
    "with open(os.path.join(path, 'jobid', 'data.json'), 'r') as f:\n",
    "    jobid_dat = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_lst = [seq[-1] for seq in bow_dat['test_targets']]\n",
    "jobid_lst = [seq[-1] for seq in jobid_dat['test_data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_lst == jobid_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167579"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobid_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
