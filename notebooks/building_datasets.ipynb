{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyfasttext import FastText\n",
    "from pprint import pprint\n",
    "import os\n",
    "import json\n",
    "from bidict import bidict\n",
    "import random\n",
    "from math import ceil, floor\n",
    "import pickle\n",
    "from bson.objectid import ObjectId\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_pickle(\"/data/rali7/Tmp/solimanz/data/pickles/excerpt-2017-02-20_transformed.pkl\")\n",
    "df = pd.read_pickle(\"/data/rali7/Tmp/solimanz/data/pickles/excerpt-2017-02-20_reduced.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_counts = df.transformed.value_counts()\n",
    "top = func_counts[:550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second cleaning method\n",
    "func_counts = df.reduced.value_counts()\n",
    "top = func_counts[:7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ids = df[~df.reduced.isin(top.index)][\"_id\"].unique()\n",
    "all_ids = df[\"_id\"].unique()\n",
    "dataset_ids = list(set(all_ids) - set(bad_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of entire dataset: 837910\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of entire dataset: {len(dataset_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = ceil(0.8 * len(dataset_ids))\n",
    "random.shuffle(dataset_ids)\n",
    "train_ids = dataset_ids[:train_size]\n",
    "test_ids = dataset_ids[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mapping between job title id and string representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = top.index.values\n",
    "title_id = {title: i for i, title in enumerate(job_titles)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep a record of the training and testing IDs for later experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/top7000/train_ids.pkl\", \"wb\")as f:\n",
    "    pickle.dump(file=f, obj=train_ids)\n",
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/top7000/test_ids.pkl\", \"wb\")as f:\n",
    "    pickle.dump(file=f, obj=test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS1: Simple Job Titles Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_path = \"/data/rali7/Tmp/solimanz/data/datasets/top7000/1/\"\n",
    "ds1_file_name = \"title_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df._id.isin(dataset_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_series = df.groupby('_id')['reduced'].apply(lambda x: list(reversed(list(x))))\n",
    "#func_series = df.groupby('_id')['transformed'].apply(lambda x: list(reversed(list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [[title_id[title] for title in func_series[i]] for i in train_ids]\n",
    "test_data = [[title_id[title] for title in func_series[i]] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_data])\n",
    "max_test_seq = max([len(seq) for seq in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Longest Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 53\n",
      "Maximum length of test sequences: 65\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq),\n",
    "        'n_lables': len(top)\n",
    "    }\n",
    "\n",
    "with open(os.path.join(ds1_path, f\"{ds1_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS2: Add Start of Sequence tags to DS1 Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2_path = \"/data/rali7/Tmp/solimanz/data/datasets/top7000/2/\"\n",
    "ds2_file_name = \"title_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ds1_path, f\"{ds1_file_name}.json\"), 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = data[\"title_to_id\"]\n",
    "train_data = data[\"train_data\"] \n",
    "test_data = data[\"test_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id['<START>'] = len(title_id)\n",
    "start_tag = title_id['<START>']\n",
    "_ =[seq.insert(0, start_tag) for seq in train_data]\n",
    "_ =[seq.insert(0, start_tag) for seq in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_data])\n",
    "max_test_seq = max([len(seq) for seq in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Longest Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 54\n",
      "Maximum length of test sequences: 66\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq)\n",
    "    }\n",
    "with open(os.path.join(ds2_path, f\"{ds2_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS3: Job Sequences as Sequences of Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we represent our job experience sequences as sequences of bag-of-words vectors that we will feed to the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3_path = \"/data/rali7/Tmp/solimanz/data/datasets/3/\"\n",
    "ds3_file_name = \"title_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/train_ids.pkl\", \"rb\")as f:\n",
    "    train_ids = pickle.load(file=f)\n",
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/test_ids.pkl\", \"rb\")as f:\n",
    "    test_ids = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = \" \".join(top_550.index.values)\n",
    "tokens = re.split(r\"[-/,\\.\\\\\\s]\", joined)\n",
    "token_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(token_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "for word in sw:\n",
    "    if word in vocab:\n",
    "        vocab.remove(word)\n",
    "if '' in vocab:\n",
    "    vocab.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_id = {l: i for i, l in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 329\n"
     ]
    }
   ],
   "source": [
    "voc_size = len(vocab_id)\n",
    "bow = {}\n",
    "print(f\"Vocabulary Size: {voc_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in func_counts.index:\n",
    "    tokens = re.split(r\"[-/,\\.\\\\\\s]\", title)\n",
    "    token_indices = [vocab_id[tok] for tok in tokens if tok in vocab_id]\n",
    "    bow[title] = sorted(token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df._id.isin(dataset_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_series = df.groupby('_id')['transformed'].apply(lambda x: list(reversed(list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = [[title for title in func_series[i]] for i in train_ids]\n",
    "test_seqs = [[title for title in func_series[i]] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "train_inputs = [[bow[title] for title in seq[:-1]] for seq in train_seqs] \n",
    "test_inputs = [[bow[title] for title in seq[:-1]] for seq in test_seqs]\n",
    "\n",
    "# Targets\n",
    "train_targets = [[title_id[title] for title in seq[1:]] for seq in train_seqs]\n",
    "test_targets = [[title_id[title] for title in seq[1:]] for seq in test_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_seqs])\n",
    "max_test_seq = max([len(seq) for seq in test_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 32\n",
      "Maximum length of test sequences: 19\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'title_to_bow': bow,\n",
    "        'vocab_id': vocab_id,\n",
    "        'train_inputs': train_inputs,\n",
    "        'test_inputs': test_inputs,\n",
    "        'train_targets': train_targets,\n",
    "        'test_targets': test_targets,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq)\n",
    "    }\n",
    "with open(os.path.join(ds3_path, f\"{ds3_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS4: Add Job Duration to DS3 Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds4_path = \"/data/rali7/Tmp/solimanz/data/datasets/4/\"\n",
    "ds4_file_name = \"title_sequences_durations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df._id.isin(dataset_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_series = df.groupby('_id')['transformed'].apply(lambda x: list(reversed(list(x))))\n",
    "duration_series = df.groupby('_id')['duration'].apply(lambda x: list(reversed(list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['director', 'instructor', 'instructor', 'substitute teacher']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_series['52b31c980b045119318b9d64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.17, 0.0, 5.83, 11.75]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_series[324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = [[title for title in func_series[i]] for i in train_ids]\n",
    "test_seqs = [[title for title in func_series[i]] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs we will concatenate the duration value with the corresponding bow in the batcher\n",
    "train_inputs = [[bow[title] for title in seq[:-1]] for seq in train_seqs]\n",
    "train_duration = [[dur for dur in duration_series[i][:-1]] for i in train_ids]\n",
    "test_inputs = [[bow[title] for title in seq[:-1]] for seq in test_seqs]\n",
    "test_duration = [[dur for dur in duration_series[i][:-1]] for i in test_ids]\n",
    "\n",
    "# Targets\n",
    "train_targets = [[title_id[title] for title in seq[1:]] for seq in train_seqs]\n",
    "test_targets = [[title_id[title] for title in seq[1:]] for seq in test_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_seqs])\n",
    "max_test_seq = max([len(seq) for seq in test_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 32\n",
      "Maximum length of test sequences: 22\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'title_to_bow': bow,\n",
    "        'vocab_id': vocab_id,\n",
    "        'train_inputs': train_inputs,\n",
    "        'train_durations': train_duration,\n",
    "        'test_inputs': test_inputs,\n",
    "        'test_duration': test_duration,\n",
    "        'train_targets': train_targets,\n",
    "        'test_targets': test_targets,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq)\n",
    "    \n",
    "    }\n",
    "with open(os.path.join(ds4_path, f\"{ds4_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS5: Input Vectors as Word Embeddings Mutli Label Targets(Fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds5_path = \"/data/rali7/Tmp/solimanz/data/datasets/5/\"\n",
    "ds5_file_name = \"title_embedding_sequences_multi_label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ds1_path, f\"{ds1_file_name}.json\"), 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(\"/data/rali7/Tmp/solimanz/data/wikipedia/wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = bidict(data[\"title_to_id\"])\n",
    "train_seqs = data[\"train_data\"]\n",
    "test_seqs = data[\"test_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(title_id), emb_dim), dtype=np.float32)\n",
    "\n",
    "for title in title_id.keys():\n",
    "    if len(title.split(\" \")) == 1:\n",
    "        vec = model.get_numpy_vector(title)\n",
    "    else:\n",
    "        vec = model.get_sentence_vector(title)\n",
    "    embeddings[title_id[title], :] = vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Targets will be represented as multilabel vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "train_inputs = [[title for title in seq[:-1]] for seq in train_seqs] \n",
    "test_inputs = [[title for title in seq[:-1]] for seq in test_seqs]\n",
    "\n",
    "# Targets\n",
    "train_targets = [[title_id.inv[title] for title in seq[1:]] for seq in train_seqs]\n",
    "test_targets = [[title_id.inv[title] for title in seq[1:]] for seq in test_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = \" \".join(title_id.keys())\n",
    "tokens = Counter(re.split(r\"[-/,\\.\\\\\\s_]\", titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = set(tokens.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "sw.append('')\n",
    "sw.append(' ')\n",
    "for word in sw:\n",
    "    if word in tokens:\n",
    "        tokens.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = {tok: i for i, tok in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 326\n",
      "Number of Titles: 550\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Tokens: {len(token_id)}\")\n",
    "print(f\"Number of Titles: {len(title_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = [[[token_id[tok] for tok in re.split(r\"[-/,\\.\\\\\\s_]\", title) if tok in token_id] \n",
    "  for title in seq] for seq in train_targets]\n",
    "test_targets = [[[token_id[tok] for tok in re.split(r\"[-/,\\.\\\\\\s_]\", title) if tok in token_id] \n",
    "  for title in seq] for seq in test_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_seqs])\n",
    "max_test_seq = max([len(seq) for seq in test_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 32\n",
      "Maximum length of test sequences: 22\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': dict(title_id),\n",
    "        'token_id': token_id,\n",
    "        'train_inputs': train_inputs,\n",
    "        'test_inputs': test_inputs,\n",
    "        'train_targets': train_targets,\n",
    "        'test_targets': test_targets,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq),\n",
    "        'emb_dim': emb_dim\n",
    "    \n",
    "    }\n",
    "with open(os.path.join(ds5_path, f\"{ds5_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(ds5_path, \"embeddings_small.npy\"), embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS6: Multi Label Data Representation (Larger Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds6_path = \"/data/rali7/Tmp/solimanz/data/datasets/6/\"\n",
    "ds6_file_name = \"big_title_embedding_sequences_multi_label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/train_ids.pkl\", \"rb\")as f:\n",
    "    train_ids = pickle.load(file=f)\n",
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/test_ids.pkl\", \"rb\")as f:\n",
    "    test_ids = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(s):\n",
    "    if pd.isna(s) or pd.isnull(s):\n",
    "        return np.nan\n",
    "    toks = re.split(r\"[-/,\\.\\\\\\s_]\", s)\n",
    "    token_indices = [tok for tok in toks if tok in token_id]\n",
    "    if token_indices:\n",
    "        return \" \".join(token_indices)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = set(re.split(r\"[-/,\\.\\\\\\s_]\", \" \".join(top_550.index.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "sw.append('')\n",
    "sw.append(' ')\n",
    "for word in sw:\n",
    "    if word in tokens:\n",
    "        tokens.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = {tok: i for i, tok in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reduced'] = df['transformed'].apply(reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2198993"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = df._id.unique()\n",
    "bad_ids = df[pd.isna(df.reduced)]._id.unique()\n",
    "len(all_ids) - len(bad_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df._id.isin(bad_ids)]\n",
    "titles = df.reduced.unique()\n",
    "dataset_ids = df._id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = {title: i for i, title in enumerate(titles)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_size = ceil(0.8 * len(dataset_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset_ids)\n",
    "train_ids = dataset_ids[:train_set_size]\n",
    "test_ids = dataset_ids[train_set_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/train_ids_big.pkl\", \"wb\")as f:\n",
    "    pickle.dump(file=f, obj=train_ids)\n",
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/test_ids_big.pkl\", \"wb\")as f:\n",
    "    pickle.dump(file=f, obj=test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_series = df.groupby('_id')['reduced'].apply(lambda x: list(reversed(list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [[title_id[title] for title in func_series[i]] for i in train_ids]\n",
    "test_data = [[title_id[title] for title in func_series[i]] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_data])\n",
    "max_test_seq = max([len(seq) for seq in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Longest Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 89\n",
      "Maximum length of test sequences: 56\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 326\n",
      "Number of Titles: 731408\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Tokens: {len(token_id)}\")\n",
    "print(f\"Number of Titles: {len(title_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(\"/data/rali7/Tmp/solimanz/data/wikipedia/wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(title_id), emb_dim), dtype=np.float32)\n",
    "\n",
    "for title in title_id.keys():\n",
    "    vec = model.get_sentence_vector(title)\n",
    "    embeddings[title_id[title], :] = vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Targets will be represented as multilabel vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = bidict(title_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "train_inputs = [[title for title in seq[:-1]] for seq in train_data] \n",
    "test_inputs = [[title for title in seq[:-1]] for seq in test_data]\n",
    "\n",
    "# Targets\n",
    "train_targets = [[title_id.inv[title] for title in seq[1:]] for seq in train_data]\n",
    "test_targets = [[title_id.inv[title] for title in seq[1:]] for seq in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = [[[token_id[tok] for tok in re.split(r\"[-/,\\.\\\\\\s_]\", title) if tok in token_id] \n",
    "  for title in seq] for seq in train_targets]\n",
    "test_targets = [[[token_id[tok] for tok in re.split(r\"[-/,\\.\\\\\\s_]\", title) if tok in token_id] \n",
    "  for title in seq] for seq in test_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': dict(title_id),\n",
    "        'token_id': token_id,\n",
    "        'train_inputs': train_inputs,\n",
    "        'test_inputs': test_inputs,\n",
    "        'train_targets': train_targets,\n",
    "        'test_targets': test_targets,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq),\n",
    "        'emb_dim': emb_dim\n",
    "    \n",
    "    }\n",
    "with open(os.path.join(ds6_path, f\"{ds6_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(ds6_path, \"embeddings_big.npy\"), embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['co owner', 'co owner', 'owner'],\n",
       " ['associate', 'partner', 'partner'],\n",
       " ['analyst',\n",
       "  'information technology consultant',\n",
       "  'project manager',\n",
       "  'program manager',\n",
       "  'director',\n",
       "  'director',\n",
       "  'director'],\n",
       " ['educator',\n",
       "  'project coordinator',\n",
       "  'associate project manager',\n",
       "  'project manager'],\n",
       " ['investment representative',\n",
       "  'operations associate',\n",
       "  'financial service representative'],\n",
       " ['sales associate', 'sales associate', 'financial services associate'],\n",
       " ['field operator', 'field sales', 'field supervisor'],\n",
       " ['security guard',\n",
       "  'sales associate',\n",
       "  'receptionist',\n",
       "  'security',\n",
       "  'security officer',\n",
       "  'administrative support assistant'],\n",
       " ['facilitator', 'counsellor', 'youth coordinator', 'addictions counsellor'],\n",
       " ['music educator', 'music educator', 'educator']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list(func_series.values), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
