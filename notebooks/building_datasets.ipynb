{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyfasttext import FastText\n",
    "from pprint import pprint\n",
    "import os\n",
    "import json\n",
    "from bidict import bidict\n",
    "import random\n",
    "from math import ceil\n",
    "import pickle\n",
    "from bson.objectid import ObjectId\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"/data/rali7/Tmp/solimanz/data/pickles/clean_2017_11_28.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_counts = df.transformed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_550 = func_counts[:550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ids = df[~df.transformed.isin(top_550.index)][\"_id\"].unique()\n",
    "all_ids = df[\"_id\"].unique()\n",
    "dataset_ids = list(set(all_ids) - set(bad_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of entire dataset: 120371\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of entire dataset: {len(dataset_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset_ids)\n",
    "train_size = ceil(0.8 * dataset_size) \n",
    "test_size = ceil(0.2 * dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = random.sample(range(dataset_size), train_size)\n",
    "test_idx = random.sample(range(dataset_size), test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [dataset_ids[i] for i in train_idx] \n",
    "test_ids = [dataset_ids[i] for i in test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mapping between job title id and string representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = top_550.index.values\n",
    "title_id = {title: i for i, title in enumerate(job_titles)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep a record of the training and testing IDs for later experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/train_ids.pkl\", \"wb\")as f:\n",
    "    pickle.dump(file=f, obj=train_ids)\n",
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/test_ids.pkl\", \"wb\")as f:\n",
    "    pickle.dump(file=f, obj=test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS1: Simple Job Titles Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_path = \"/data/rali7/Tmp/solimanz/data/datasets/1/\"\n",
    "ds1_file_name = \"title_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[data._id.isin(dataset_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_series = df.groupby('_id')['transformed'].apply(lambda x: list(reversed(list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [[title_id[title] for title in func_series[i]] for i in train_ids]\n",
    "test_data = [[title_id[title] for title in func_series[i]] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_data])\n",
    "max_test_seq = max([len(seq) for seq in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Longest Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 24\n",
      "Maximum length of test sequences: 32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq)\n",
    "    }\n",
    "\n",
    "with open(os.path.join(ds1_path, f\"{ds1_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS2: Add Start of Sequence tags to DS1 Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2_path = \"/data/rali7/Tmp/solimanz/data/datasets/2/\"\n",
    "ds2_file_name = \"title_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ds1_path, f\"{ds1_file_name}.json\"), 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = data[\"title_to_id\"]\n",
    "train_data = data[\"train_data\"] \n",
    "test_data = data[\"test_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id['<START>'] = len(title_id)\n",
    "start_tag = title_id['<START>']\n",
    "_ =[seq.insert(0, start_tag) for seq in train_data]\n",
    "_ =[seq.insert(0, start_tag) for seq in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_data])\n",
    "max_test_seq = max([len(seq) for seq in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Longest Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 25\n",
      "Maximum length of test sequences: 33\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq)\n",
    "    }\n",
    "with open(os.path.join(ds2_path, f\"{ds2_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS3: Job Sequences as Sequences of Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we represent our job experience sequences as sequences of bag-of-words vectors that we will feed to the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3_path = \"/data/rali7/Tmp/solimanz/data/datasets/3/\"\n",
    "ds3_file_name = \"title_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/train_ids.pkl\", \"rb\")as f:\n",
    "    train_ids = pickle.load(file=f)\n",
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/test_ids.pkl\", \"rb\")as f:\n",
    "    test_ids = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = \" \".join(top_550.index.values)\n",
    "tokens = re.split(r\"[-/,\\.\\\\\\s]\", joined)\n",
    "token_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(token_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "for word in sw:\n",
    "    if word in vocab:\n",
    "        vocab.remove(word)\n",
    "if '' in vocab:\n",
    "    vocab.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_id = {l: i for i, l in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 329\n"
     ]
    }
   ],
   "source": [
    "voc_size = len(vocab_id)\n",
    "bow = {}\n",
    "print(f\"Vocabulary Size: {voc_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in func_counts.index:\n",
    "    tokens = re.split(r\"[-/,\\.\\\\\\s]\", title)\n",
    "    token_indices = [vocab_id[tok] for tok in tokens if tok in vocab_id]\n",
    "    bow[title] = sorted(token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df._id.isin(dataset_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_series = df.groupby('_id')['transformed'].apply(lambda x: list(reversed(list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = [[title for title in func_series[i]] for i in train_ids]\n",
    "test_seqs = [[title for title in func_series[i]] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "train_inputs = [[bow[title] for title in seq[:-1]] for seq in train_seqs] \n",
    "test_inputs = [[bow[title] for title in seq[:-1]] for seq in test_seqs]\n",
    "\n",
    "# Targets\n",
    "train_targets = [[title_id[title] for title in seq[1:]] for seq in train_seqs]\n",
    "test_targets = [[title_id[title] for title in seq[1:]] for seq in test_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_seqs])\n",
    "max_test_seq = max([len(seq) for seq in test_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 24\n",
      "Maximum length of test sequences: 32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'title_to_bow': bow,\n",
    "        'vocab_id': vocab_id,\n",
    "        'train_inputs': train_inputs,\n",
    "        'test_inputs': test_inputs,\n",
    "        'train_targets': train_targets,\n",
    "        'test_targets': test_targets,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq)\n",
    "    }\n",
    "with open(os.path.join(ds3_path, f\"{ds3_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS4: Add Job Duration to DS3 Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds4_path = \"/data/rali7/Tmp/solimanz/data/datasets/4/\"\n",
    "ds4_file_name = \"title_sequences_durations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df._id.isin(dataset_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_series = df.groupby('_id')['transformed'].apply(lambda x: list(reversed(list(x))))\n",
    "duration_series = df.groupby('_id')['duration'].apply(lambda x: list(reversed(list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['director', 'instructor', 'instructor', 'substitute teacher']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_series['52b31c980b045119318b9d64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.17, 0.0, 5.83, 11.75]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_series[324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = [[title for title in func_series[i]] for i in train_ids]\n",
    "test_seqs = [[title for title in func_series[i]] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs we will concatenate the duration value with the corresponding bow in the batcher\n",
    "train_inputs = [[bow[title] for title in seq[:-1]] for seq in train_seqs]\n",
    "train_duration = [[dur for dur in duration_series[i][:-1]] for i in train_ids]\n",
    "test_inputs = [[bow[title] for title in seq[:-1]] for seq in test_seqs]\n",
    "test_duration = [[dur for dur in duration_series[i][:-1]] for i in test_ids]\n",
    "\n",
    "# Targets\n",
    "train_targets = [[title_id[title] for title in seq[1:]] for seq in train_seqs]\n",
    "test_targets = [[title_id[title] for title in seq[1:]] for seq in test_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_seqs])\n",
    "max_test_seq = max([len(seq) for seq in test_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 24\n",
      "Maximum length of test sequences: 32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'title_to_bow': bow,\n",
    "        'vocab_id': vocab_id,\n",
    "        'train_inputs': train_inputs,\n",
    "        'train_durations': train_duration,\n",
    "        'test_inputs': test_inputs,\n",
    "        'test_duration': test_duration,\n",
    "        'train_targets': train_targets,\n",
    "        'test_targets': test_targets,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq)\n",
    "    \n",
    "    }\n",
    "with open(os.path.join(ds4_path, f\"{ds4_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS5: Input Vectors as Word Embeddings Mutli Label Targets(Fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds5_path = \"/data/rali7/Tmp/solimanz/data/datasets/5/\"\n",
    "ds5_file_name = \"title_embedding_sequences_multi_label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ds1_path, f\"{ds1_file_name}.json\"), 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(\"/data/rali7/Tmp/solimanz/data/wikipedia/wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = bidict(data[\"title_to_id\"])\n",
    "train_seqs = data[\"train_data\"]\n",
    "test_seqs = data[\"test_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(title_id), emb_dim), dtype=np.float32)\n",
    "\n",
    "for title in title_id.keys():\n",
    "    if len(title.split(\" \")) == 1:\n",
    "        vec = model.get_numpy_vector(title)\n",
    "    else:\n",
    "        vec = model.get_sentence_vector(title)\n",
    "    embeddings[title_id[title], :] = vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Targets will be represented as multilabel vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "train_inputs = [[title for title in seq[:-1]] for seq in train_seqs] \n",
    "test_inputs = [[title for title in seq[:-1]] for seq in test_seqs]\n",
    "\n",
    "# Targets\n",
    "train_targets = [[title_id.inv[title] for title in seq[1:]] for seq in train_seqs]\n",
    "test_targets = [[title_id.inv[title] for title in seq[1:]] for seq in test_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = \" \".join(title_id.keys())\n",
    "tokens = Counter(re.split(r\"[-/,\\.\\\\\\s_]\", titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = set(tokens.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "sw.append('')\n",
    "sw.append(' ')\n",
    "for word in sw:\n",
    "    if word in tokens:\n",
    "        tokens.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = {tok: i for i, tok in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 326\n",
      "Number of Titles: 550\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Tokens: {len(token_id)}\")\n",
    "print(f\"Number of Titles: {len(title_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = [[[token_id[tok] for tok in re.split(r\"[-/,\\.\\\\\\s_]\", title) if tok in token_id] \n",
    "  for title in seq] for seq in train_targets]\n",
    "test_targets = [[[token_id[tok] for tok in re.split(r\"[-/,\\.\\\\\\s_]\", title) if tok in token_id] \n",
    "  for title in seq] for seq in test_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_seqs])\n",
    "max_test_seq = max([len(seq) for seq in test_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 24\n",
      "Maximum length of test sequences: 32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': dict(title_id),\n",
    "        'token_id': token_id,\n",
    "        'train_inputs': train_inputs,\n",
    "        'test_inputs': test_inputs,\n",
    "        'train_targets': train_targets,\n",
    "        'test_targets': test_targets,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq),\n",
    "        'emb_dim': emb_dim\n",
    "    \n",
    "    }\n",
    "with open(os.path.join(ds5_path, f\"{ds5_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(ds5_path, \"embeddings_small.npy\"), embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS6: Multi Label Data Representation (Larger Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds6_path = \"/data/rali7/Tmp/solimanz/data/datasets/6/\"\n",
    "ds6_file_name = \"multilabel_title_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(s):\n",
    "    if pd.isna(s) or pd.isnull(s):\n",
    "        return np.nan\n",
    "    tokens = re.split(r\"[-/,\\.\\\\\\s]\", s)\n",
    "    token_indices = [tok for tok in tokens if tok in label_id]\n",
    "    if token_indices:\n",
    "        return \" \".join(token_indices)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['reduced'] = data[\"transformed\"].apply(reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = set(all_ids) - set(bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = list(good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_func = data[data._id.isin(good)].reduced.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_titles = reduced_func.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = np.zeros((5, 5, 331), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, bow in enumerate(train_inputs[0]):\n",
    "    input_seqs[0][i][bow] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[53], [125], [125]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 19, 19]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.random.randint(876, size=(10,34,76))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t[p, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inputs) == len(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(train_inputs, train_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[140], [42], [42]],\n",
       " [[202], [1, 100], [1, 100]],\n",
       " [[100, 141], [195, 327], [100, 141]],\n",
       " [[46, 118], [46, 118], [118]],\n",
       " [[143], [195, 305], [264]],\n",
       " [[114], [114]],\n",
       " [[190, 233, 310], [100, 141]],\n",
       " [[29, 292], [246]],\n",
       " [[100, 195], [195, 310], [195, 310]],\n",
       " [[55, 275], [195]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
