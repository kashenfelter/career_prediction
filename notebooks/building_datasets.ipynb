{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyfasttext import FastText\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "from bidict import bidict\n",
    "import random\n",
    "from math import ceil, floor\n",
    "import pickle\n",
    "from bson.objectid import ObjectId\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import ftfy\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"/data/rali7/Tmp/solimanz/data/pickles/excerpt-2017-02-20_reduced.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = df.company_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "def dump(path, serializer, obj):\n",
    "    \"\"\"\n",
    "    Saves 'obj' to 'path' using 'serializer' (either pickle or json)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(os.path.dirname(path)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(path))\n",
    "        except OSError as exc:  # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    mode = \"wb\" if serializer.__name__ == \"pickle\" else \"w\"\n",
    "    with open(path, mode) as f:\n",
    "        serializer.dump(f, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(n=550, col='transformed'):\n",
    "    top = df[col].value_counts()[:n]\n",
    "    bad_ids = df[~df[col].isin(top.index)][\"_id\"].unique()\n",
    "    all_ids = df[\"_id\"].unique()\n",
    "    dataset_ids = list(set(all_ids) - set(bad_ids))\n",
    "    \n",
    "    return dataset_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset_ids, seed=123):\n",
    "    random.seed(seed)\n",
    "    train_size = ceil(0.8 * len(dataset_ids))\n",
    "    random.shuffle(dataset_ids)    \n",
    "    train_ids = dataset_ids[:train_size]\n",
    "    test_ids = dataset_ids[train_size:]\n",
    "    \n",
    "    valid_size = ceil(0.2 * len(train_ids))\n",
    "    random.shuffle(train_ids)\n",
    "    valid_ids = train_ids[:valid_size]\n",
    "    train_ids = train_ids[valid_size:]\n",
    "    \n",
    "    return train_ids, valid_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ids(train_ids, valid_ids, test_ids, path=\"/data/rali7/Tmp/solimanz/LBJ/dataset\"):\n",
    "    with open(os.path.join(path, 'train', 'train_ids.pkl'), \"wb\")as f:\n",
    "        pickle.dump(file=f, obj=train_ids)\n",
    "    with open(os.path.join(path, 'valid', 'valid_ids.pkl'), \"wb\")as f:\n",
    "        pickle.dump(file=f, obj=valid_ids)\n",
    "    with open(os.path.join(path, 'test', 'test_ids.pkl'), \"wb\")as f:\n",
    "        pickle.dump(file=f, obj=test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ids = get_ids(n=550, col='reduced')\n",
    "train_ids, valid_ids, test_ids = split_data(dataset_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of entire dataset: 266285\n",
      "Size of train dataset: 170422\n",
      "Size of valid dataset: 42606\n",
      "Size of test dataset: 53257\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of entire dataset: {len(dataset_ids)}\")\n",
    "print(f\"Size of train dataset: {len(train_ids)}\")\n",
    "print(f\"Size of valid dataset: {len(valid_ids)}\")\n",
    "print(f\"Size of test dataset: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep a record of the training and testing IDs for later experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ids(train_ids, valid_ids, test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ids(path=\"/data/rali7/Tmp/solimanz/LBJ/dataset\"):\n",
    "    with open(os.path.join(path, 'train', 'train_ids.pkl'), \"rb\")as f:\n",
    "        train_ids = pickle.load(f)\n",
    "    with open(os.path.join(path, 'valid', 'valid_ids.pkl'), \"rb\")as f:\n",
    "        valid_ids = pickle.load(f)\n",
    "    with open(os.path.join(path, 'test', 'test_ids.pkl'), \"rb\")as f:\n",
    "        test_ids = pickle.load(f)\n",
    "        \n",
    "    return train_ids, valid_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, valid_ids, test_ids = load_ids()\n",
    "dataset_ids = train_ids + valid_ids + test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df._id.isin(dataset_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = df.reduced.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_series = df.groupby('_id')['reduced'].apply(lambda x: list(reversed(list(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mapping between job title id and string representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = {title: i for i, title in enumerate(job_titles)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# train_data = set(itertools.chain.from_iterable([func_series[i] for i in train_ids]))\n",
    "# valid_data = set(itertools.chain.from_iterable([func_series[i] for i in valid_ids]))\n",
    "# test_data = set(itertools.chain.from_iterable([func_series[i] for i in test_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = [[i, [title_id[func] for func in func_series[i]]] for i in train_ids]\n",
    "valid_seq = [[i, [title_id[func] for func in func_series[i]]] for i in valid_ids]\n",
    "test_seq = [[i, [title_id[func] for func in func_series[i]]] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = [{\"sequence\": \">\".join(func_series[i][:-1]), \"labels\": \">\".join(func_series[i][1:])} for i in train_ids]\n",
    "# valid_data = [{\"sequence\": \">\".join(func_series[i][:-1]), \"labels\": \">\".join(func_series[i][1:])} for i in valid_ids]\n",
    "# test_data = [{\"sequence\": \">\".join(func_series[i][:-1]), \"labels\": \">\".join(func_series[i][1:])} for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(dat[1]) for dat in train_seq])\n",
    "max_valid_seq = max([len(dat[1]) for dat in valid_seq])\n",
    "max_test_seq = max([len(dat[1]) for dat in test_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 32\n",
      "Maximum length of valid sequences : 31\n",
      "Maximum length of test sequences: 31\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Maximum length of training sequences : {max_train_seq}\n",
    "Maximum length of valid sequences : {max_valid_seq}\n",
    "Maximum length of test sequences: {max_test_seq}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dump JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/data/rali7/Tmp/solimanz/LBJ/dataset/\"\n",
    "\n",
    "train_data = {\n",
    "        'sequences': train_seq,\n",
    "        'maximum_seq_len':max_train_seq,\n",
    "        'title_id': dict(title_id),\n",
    "        'n_labels': len(title_id)\n",
    "    }\n",
    "\n",
    "valid_data = {\n",
    "        'sequences': valid_seq,\n",
    "        'maximum_seq_len': max_valid_seq,\n",
    "        'title_id': dict(title_id),\n",
    "        'n_labels': len(title_id)\n",
    "    }\n",
    "\n",
    "test_data = {\n",
    "        'sequences': test_seq,\n",
    "        'maximum_seq_len': max_test_seq,\n",
    "        'title_id': dict(title_id),\n",
    "        'n_labels': len(title_id)\n",
    "    }\n",
    "\n",
    "# data = {\n",
    "#         'train_data': train_data,\n",
    "#         'valid_data': valid_data,\n",
    "#         'test_data': test_data,\n",
    "#         'maximum_seq_len': max(max_train_seq, max_valid_seq, max_test_seq),\n",
    "#         'title_id': title_id,\n",
    "#         'n_labels': len(title_id)\n",
    "#     }\n",
    "\n",
    "# with open(os.path.join(data_path, 'train', 'train.json'), 'w') as f:\n",
    "#     for d in train_data:\n",
    "#         json_data = json.dumps(d)\n",
    "#         f.write(json_data)\n",
    "#         f.write('\\n')\n",
    "# with open(os.path.join(data_path, 'valid', 'valid.json'), 'w') as f:\n",
    "#     for d in valid_data:\n",
    "#         json_data = json.dumps(d)\n",
    "#         f.write(json_data)\n",
    "#         f.write('\\n')\n",
    "# with open(os.path.join(data_path, 'test', 'test.json'), 'w') as f:\n",
    "#     for d in test_data:\n",
    "#         json_data = json.dumps(d)\n",
    "#         f.write(json_data)\n",
    "#         f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, 'train', 'train.json'), 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "with open(os.path.join(data_path, 'valid', 'valid.json'), 'w') as f:\n",
    "    json.dump(valid_data, f)\n",
    "with open(os.path.join(data_path, 'test', 'test.json'), 'w') as f:\n",
    "    json.dump(test_data, f)\n",
    "# with open(os.path.join(data_path, 'data.json'), 'w') as f:\n",
    "#     json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Vectors as Word Embeddings Mutli Label Targets(Fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(\"/data/rali7/Tmp/solimanz/LBJ/crawl-300d-2M-subword.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = bidict(title_id)\n",
    "# train_seqs = data[\"train_data\"]\n",
    "# valid_seqs = data[\"valid_data\"]\n",
    "# test_seqs = data[\"test_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(title_id), emb_dim), dtype=np.float32)\n",
    "\n",
    "for title in title_id.keys():\n",
    "    vec = model.get_sentence_vector(title)\n",
    "    embeddings[title_id[title], :] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(\"/data/rali7/Tmp/solimanz/LBJ/dataset/\", \"embeddings_reduced.npy\"), embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skill Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, 'skills.pkl'), 'rb') as f:\n",
    "    skills = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skills = []\n",
    "for v in skills.values():\n",
    "    if v:\n",
    "        all_skills += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skills = [skill.lower() for skill in all_skills]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_counts = Counter(all_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_ids(id_skills):\n",
    "    \"\"\" Returns dict that associates every token to an id\"\"\"\n",
    "    sw = set(stopwords.words('english') + ['...'])\n",
    "    tokenizer = RegexpTokenizer(r'\\s+|\\W', gaps=True) \n",
    "    skills = []    \n",
    "    for s in id_skills.values():\n",
    "        if s:\n",
    "            skills += s\n",
    "    \n",
    "    tokens = list(set(tokenizer.tokenize(\" \".join(skills))))\n",
    "    tokens = [t.lower() for t in tokens if t not in string.punctuation and t not in sw]\n",
    "    \n",
    "    return {token: id_ for id_, token in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skill_tokens(id_skills):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\s+|\\W', gaps=True)    \n",
    "    for key, skills in id_skills.items():\n",
    "        if skills:\n",
    "            tokens = tokenizer.tokenize(\" \".join(skills))\n",
    "            id_skills[key] = tuple(set(tokens))\n",
    "        \n",
    "    return id_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_skills = {k: sorted([s.lower() for skillset], key=lambda s: skill_counts[s], reverse=True)[:10] for k, skillset in skills.items() if skillset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_skills = []\n",
    "for v in id_skills.values():\n",
    "    all_skills += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_counts = Counter(all_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_id = {skill: i for i, skill in enumerate(skill_counts.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_id['<null>'] = len(skill_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(skill_id), emb_dim), dtype=np.float32)\n",
    "for skill in skill_id.keys():\n",
    "    if not skill == '<null>':\n",
    "        vec = model.get_sentence_vector(skill)\n",
    "        embeddings[skill_id[skill], :] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42405, 300)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/data/rali7/Tmp/solimanz/LBJ/dataset/skill_emb.npy\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [len(s) for s in id_skills.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, 'train', 'train.json'), 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open(os.path.join(data_path, 'valid', 'valid.json'), 'r') as f:\n",
    "    valid_data = json.load(f)\n",
    "with open(os.path.join(data_path, 'test', 'test.json'), 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(data):\n",
    "    for seq in data['sequences']:\n",
    "        if seq[0] in id_skills:\n",
    "            seq.append([skill_id[s] for s in id_skills[seq[0]]])\n",
    "        else:\n",
    "            seq.append([skill_id['<null>']])\n",
    "    \n",
    "    data['max_n_skills'] = 10\n",
    "    data['emb_path'] = \"/data/rali7/Tmp/solimanz/LBJ/dataset/skill_emb.npy\"\n",
    "    data['skill_id'] = skill_id\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = augment_data(train_data)\n",
    "valid_data = augment_data(valid_data)\n",
    "test_data = augment_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, 'train', 'train.json'), 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "with open(os.path.join(data_path, 'valid', 'valid.json'), 'w') as f:\n",
    "    json.dump(valid_data, f)\n",
    "with open(os.path.join(data_path, 'test', 'test.json'), 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sequences', 'maximum_seq_len', 'title_id', 'n_labels', 'max_n_skills', 'emb_path', 'skill_id'])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Education Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, 'education.pkl'), 'rb') as f:\n",
    "    education = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu = defaultdict(list)\n",
    "for key, edu_list in education.items():\n",
    "    if not edu_list:\n",
    "        edu[key] = None\n",
    "    else:\n",
    "        for e in edu_list:\n",
    "            if 'schoolName' in e:\n",
    "                edu[key].append(e['schoolName'])\n",
    "            if 'name'in e:\n",
    "                edu[key].append(e['name'])\n",
    "            if 'sector' in e:\n",
    "                edu[key].append(e['sector'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
