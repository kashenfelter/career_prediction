{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import os\n",
    "import json\n",
    "from bidict import bidict\n",
    "import random\n",
    "from math import ceil\n",
    "import pickle\n",
    "from bson.objectid import ObjectId\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"/data/rali7/Tmp/solimanz/data/pickles/clean_2017_11_28.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_counts = df.transformed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_550 = func_counts[:550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ids = df[~df.transformed.isin(top_550.index)][\"_id\"].unique()\n",
    "all_ids = df[\"_id\"].unique()\n",
    "dataset_ids = list(set(all_ids) - set(bad_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of entire dataset: 120371\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of entire dataset: {len(dataset_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset_ids)\n",
    "train_size = ceil(0.8 * dataset_size) \n",
    "test_size = ceil(0.2 * dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = random.sample(range(dataset_size), train_size)\n",
    "test_idx = random.sample(range(dataset_size), test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [dataset_ids[i] for i in train_idx] \n",
    "test_ids = [dataset_ids[i] for i in test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mapping between job title id and string representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = top_550.index.values\n",
    "title_id = {title: i for i, title in enumerate(job_titles)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep a record of the training and testing IDs for later experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/train_ids.pkl\", \"wb\")as f:\n",
    "    pickle.dump(file=f, obj=train_ids)\n",
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/test_ids.pkl\", \"wb\")as f:\n",
    "    pickle.dump(file=f, obj=test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS1: Simple Job Titles Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_path = \"/data/rali7/Tmp/solimanz/data/datasets/1/\"\n",
    "ds1_file_name = \"title_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[data._id.isin(dataset_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_series = df.groupby('_id')['transformed'].apply(lambda x: list(reversed(list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [[title_id[title] for title in func_series[i]] for i in train_ids]\n",
    "test_data = [[title_id[title] for title in func_series[i]] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_data])\n",
    "max_test_seq = max([len(seq) for seq in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Longest Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 24\n",
      "Maximum length of test sequences: 32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq)\n",
    "    }\n",
    "\n",
    "with open(os.path.join(ds1_path, f\"{ds1_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS2: Add Start of Sequence tags to DS1 Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2_path = \"/data/rali7/Tmp/solimanz/data/datasets/2/\"\n",
    "ds2_file_name = \"title_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ds1_path, f\"{ds1_file_name}.json\"), 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = data[\"title_to_id\"]\n",
    "train_data = data[\"train_data\"] \n",
    "test_data = data[\"test_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id['<START>'] = len(title_id)\n",
    "start_tag = title_id['<START>']\n",
    "_ =[seq.insert(0, start_tag) for seq in train_data]\n",
    "_ =[seq.insert(0, start_tag) for seq in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_data])\n",
    "max_test_seq = max([len(seq) for seq in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Longest Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 25\n",
      "Maximum length of test sequences: 33\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq)\n",
    "    }\n",
    "with open(os.path.join(ds2_path, f\"{ds2_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS3: Job Sequences as Sequences of Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we represent our job experience sequences as sequences of bag-of-words vectors that we will feed to the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3_path = \"/data/rali7/Tmp/solimanz/data/datasets/3/\"\n",
    "ds3_file_name = \"title_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/train_ids.pkl\", \"rb\")as f:\n",
    "    train_ids = pickle.load(file=f)\n",
    "with open(\"/data/rali7/Tmp/solimanz/data/datasets/test_ids.pkl\", \"rb\")as f:\n",
    "    test_ids = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = \" \".join(top_550.index.values)\n",
    "tokens = re.split(r\"[-/,\\.\\\\\\s]\", joined)\n",
    "token_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(token_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sw:\n",
    "    if word in vocab:\n",
    "        vocab.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '' in vocab:\n",
    "    vocab.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_id = {l: i for i, l in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 329\n"
     ]
    }
   ],
   "source": [
    "voc_size = len(vocab_id)\n",
    "bow = {}\n",
    "print(f\"Vocabulary Size: {voc_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in func_counts.index:\n",
    "    tokens = re.split(r\"[-/,\\.\\\\\\s]\", title)\n",
    "    token_indices = [vocab_id[tok] for tok in tokens if tok in vocab_id]\n",
    "    bow[title] = sorted(token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df._id.isin(dataset_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_series = df.groupby('_id')['transformed'].apply(lambda x: list(reversed(list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = [[title for title in func_series[i]] for i in train_ids]\n",
    "test_seqs = [[title for title in func_series[i]] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "train_inputs = [[bow[title] for title in seq[:-1]] for seq in train_seqs] \n",
    "test_inputs = [[bow[title] for title in seq[:-1]] for seq in test_seqs]\n",
    "\n",
    "# Targets\n",
    "train_targets = [[title_id[title] for title in seq[1:]] for seq in train_seqs]\n",
    "test_targets = [[title_id[title] for title in seq[1:]] for seq in test_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_seqs])\n",
    "max_test_seq = max([len(seq) for seq in test_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 24\n",
      "Maximum length of test sequences: 32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'title_to_bow': bow,\n",
    "        'vocab_id': vocab_id,\n",
    "        'train_inputs': train_inputs,\n",
    "        'test_inputs': test_inputs,\n",
    "        'train_targets': train_targets,\n",
    "        'test_targets': test_targets,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq)\n",
    "    }\n",
    "with open(os.path.join(ds3_path, f\"{ds3_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS4: Add Job Duration to DS3 Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds4_path = \"/data/rali7/Tmp/solimanz/data/datasets/4/\"\n",
    "ds4_file_name = \"title_sequences_durations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute '_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e3cce85f2363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute '_id'"
     ]
    }
   ],
   "source": [
    "df = df[df._id.isin(dataset_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_series = df.groupby('_id')['transformed'].apply(lambda x: list(reversed(list(x))))\n",
    "duration_series = df.groupby('_id')['duration'].apply(lambda x: list(reversed(list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['director', 'instructor', 'instructor', 'substitute teacher']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_series['52b31c980b045119318b9d64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.17, 0.0, 5.83, 11.75]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_series[324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = [[title for title in func_series[i]] for i in train_ids]\n",
    "test_seqs = [[title for title in func_series[i]] for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs we will concatenate the duration value with the corresponding bow in the batcher\n",
    "train_inputs = [[bow[title] for title in seq[:-1]] for seq in train_seqs]\n",
    "train_duration = [[dur for dur in duration_series[i][:-1]] for i in train_ids]\n",
    "test_inputs = [[bow[title] for title in seq[:-1]] for seq in test_seqs]\n",
    "test_duration = [[dur for dur in duration_series[i][:-1]] for i in test_ids]\n",
    "\n",
    "# Targets\n",
    "train_targets = [[title_id[title] for title in seq[1:]] for seq in train_seqs]\n",
    "test_targets = [[title_id[title] for title in seq[1:]] for seq in test_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_seq = max([len(seq) for seq in train_seqs])\n",
    "max_test_seq = max([len(seq) for seq in test_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of training sequences : 24\n",
      "Maximum length of test sequences: 32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum length of training sequences : {max_train_seq}\\nMaximum length of test sequences: {max_test_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'title_to_id': title_id,\n",
    "        'title_to_bow': bow,\n",
    "        'vocab_id': vocab_id,\n",
    "        'train_inputs': train_inputs,\n",
    "        'train_durations': train_duration,\n",
    "        'test_inputs': test_inputs,\n",
    "        'test_duration': test_duration,\n",
    "        'train_targets': train_targets,\n",
    "        'test_targets': test_targets,\n",
    "        'maximum_seq_len': max(max_train_seq, max_test_seq)\n",
    "    \n",
    "    }\n",
    "with open(os.path.join(ds4_path, f\"{ds4_file_name}.json\"), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS5: Input Vectors as Word Embeddings (Fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds5_path = \"/data/rali7/Tmp/solimanz/data/datasets/5/\"\n",
    "ds5_file_name = \"title_embedding_sequences\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS6: Multi Label Data Representation (Larger Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds6_path = \"/data/rali7/Tmp/solimanz/data/datasets/6/\"\n",
    "ds6_file_name = \"multilabel_title_sequences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(s):\n",
    "    if pd.isna(s) or pd.isnull(s):\n",
    "        return np.nan\n",
    "    tokens = re.split(r\"[-/,\\.\\\\\\s]\", s)\n",
    "    token_indices = [tok for tok in tokens if tok in label_id]\n",
    "    if token_indices:\n",
    "        return \" \".join(token_indices)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['reduced'] = data[\"transformed\"].apply(reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = set(all_ids) - set(bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = list(good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_func = data[data._id.isin(good)].reduced.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_titles = reduced_func.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = np.zeros((5, 5, 331), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, bow in enumerate(train_inputs[0]):\n",
    "    input_seqs[0][i][bow] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[53], [125], [125]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 19, 19]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((4,5), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [4,23,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2, :len(x)] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 4, 23,  9,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1] = 988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 988, 9]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 4, 23,  9,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
